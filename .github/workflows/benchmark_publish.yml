name: Benchmark-Publish

# Publishes benchmark results to GitHub Pages for historical tracking and visualization.
# Results are stored per-branch at: https://<org>.github.io/<repo>/dev/bench/<branch>/

on:
  push:
    branches:
      - main
      - "main-v*"
      - "release-*"
    # Paths aligned with committer_ci.yml to trigger on relevant changes.
    # TODO(dan): Consider adding a scheduled nightly run (e.g., schedule: cron: "0 2 * * *")
    # to catch performance regressions from indirect changes like dependency updates
    # in upstream crates or Rust toolchain updates that don't touch these paths.
    paths:
      - ".github/workflows/benchmark_publish.yml"
      - "Cargo.toml"
      - "Cargo.lock"
      - "crates/starknet_committer_and_os_cli/**"
      - "crates/starknet_api/**"
      - "crates/starknet_committer/**"
      - "crates/starknet_patricia/**"
  workflow_dispatch:
    inputs:
      branch:
        description: "Branch to benchmark (leave empty for current)"
        required: false
        type: string

env:
  RUSTFLAGS: "-D warnings"

# Ensure only one benchmark job runs at a time per branch to avoid gh-pages conflicts.
concurrency:
  group: benchmark-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  benchmark:
    runs-on: namespace-profile-medium-ubuntu-24-04-amd64
    permissions:
      contents: write # Required for pushing to gh-pages

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch || github.ref }}

      # Setup pypy and link to the location expected by .cargo/config.toml.
      # Python + requirements are needed to compile the OS.
      - name: Setup PyPy
        uses: actions/setup-python@v5
        id: setup-pypy
        with:
          python-version: "pypy3.9"
          cache: "pip"

      - name: Link PyPy
        run: ln -s '${{ steps.setup-pypy.outputs.python-path }}' /usr/local/bin/pypy3.9

      - name: Set LD_LIBRARY_PATH
        run: echo "LD_LIBRARY_PATH=${{ env.Python3_ROOT_DIR }}/bin" >> $GITHUB_ENV

      - name: Install Python dependencies
        run: pip install -r scripts/requirements.txt

      # Install rust toolchain and dependencies.
      - name: Bootstrap
        uses: ./.github/actions/bootstrap
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}

      # Authenticate with GCS for downloading benchmark inputs.
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.COMMITER_PRODUCTS_EXT_WRITER_JSON }}

      - name: Setup gcloud CLI
        uses: google-github-actions/setup-gcloud@v2

      # Run benchmarks using bench_tools (downloads inputs from GCS and runs cargo bench).
      # Output is captured for github-action-benchmark to parse.
      - name: Run benchmarks
        run: |
          cargo run -p bench_tools -- run \
            --package starknet_committer_and_os_cli \
            --out /tmp/benchmark_results 2>&1 | tee benchmark_output_raw.txt
          # Extract benchmark results and convert to JSON format for github-action-benchmark.
          # Criterion outputs: "name     time:   [low mid high]" - we extract the mid value.
          grep -E "^[a-z_]+\s+time:" benchmark_output_raw.txt | while read -r line; do
            name=$(echo "$line" | awk '{print $1}')
            # Extract middle value (e.g., "17.055 ms" from "[16.676 ms 17.055 ms 17.441 ms]")
            value=$(echo "$line" | grep -oP '\[\K[^\]]+' | awk '{print $3}')
            unit=$(echo "$line" | grep -oP '\[\K[^\]]+' | awk '{print $4}')
            echo "{\"name\": \"$name\", \"unit\": \"$unit\", \"value\": $value}"
          done | jq -s '.' > benchmark_output.json
          if [ ! -s benchmark_output.json ] || [ "$(cat benchmark_output.json)" = "[]" ]; then
            echo "Error: No benchmark results found in output"
            exit 1
          fi
          echo "Benchmark results:"
          cat benchmark_output.json

      # Sanitize branch name for use as directory path (replace / with -).
      - name: Sanitize branch name
        id: branch
        run: |
          BRANCH_NAME="${{ inputs.branch || github.ref_name }}"
          SANITIZED="${BRANCH_NAME//\//-}"
          echo "name=${SANITIZED}" >> $GITHUB_OUTPUT
          echo "Branch: ${BRANCH_NAME} -> Directory: ${SANITIZED}"

      # Clean up downloaded test inputs to avoid conflicts when switching to gh-pages.
      - name: Clean up test inputs
        run: git checkout -- crates/starknet_committer_and_os_cli/test_inputs/

      # Store benchmark results to GitHub Pages.
      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: "customSmallerIsBetter"
          output-file-path: benchmark_output.json
          benchmark-data-dir-path: dev/bench/${{ steps.branch.outputs.name }}
          gh-pages-branch: gh-pages
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Alert if performance regresses by more than 15%.
          alert-threshold: "115%"
          # Comment on commit if threshold exceeded.
          comment-on-alert: true
          fail-on-alert: false

      # Upload raw benchmark results as artifact for debugging.
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ steps.branch.outputs.name }}-${{ github.sha }}
          path: |
            benchmark_output_raw.txt
            benchmark_output.json
            target/criterion/*/new/estimates.json
          retention-days: 30
